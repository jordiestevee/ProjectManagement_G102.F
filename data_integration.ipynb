{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Processing Pipeline ---\n",
      "✓ Step 0: Data loaded successfully.\n",
      "  - 'consum_dades' shape: (14439, 3)\n",
      "  - 'perimetre_dades' shape: (10, 6)\n",
      "\n",
      "--- Step 1: Merging Datasets ---\n",
      "✓ Datasets merged on 'POLISSA_SUBM'. New shape: (14439, 8)\n",
      "\n",
      "--- Step 2: Verifying Integrity and Cleaning ---\n",
      "  - Total missing values: 0\n",
      "  - Removed 0 duplicate records (same meter & date).\n",
      "  - Removed 0 invalid records (negative consumption).\n",
      "✓ Data cleaned. Final shape: (14439, 8)\n",
      "\n",
      "--- Step 3: Assessing Completeness ---\n",
      "✓ Converted 'DATA' and 'DATA_INST_COMP' to datetime objects.\n",
      "\n",
      "  - Daily consumption data ranges from: 2021-01-01 to 2024-12-31\n",
      "\n",
      "  - Record count by year:\n",
      "YEAR\n",
      "2021    3602\n",
      "2022    3605\n",
      "2023    3644\n",
      "2024    3588\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Step 5: Final Output ---\n",
      "✓ Unified, clean, and enhanced dataset is ready in the 'unified_clean_df' DataFrame.\n",
      "\n",
      "--- Final DataFrame Head ---\n",
      "       POLISSA_SUBM       DATA  CONSUM       NUM_COMPLET DATA_INST_COMP  \\\n",
      "0  4XFL2NAR75V6CQIG 2021-01-01   231.0  IBU3QNDABEVUQA7J     2016-07-20   \n",
      "1  4XFL2NAR75V6CQIG 2021-01-02   176.0  IBU3QNDABEVUQA7J     2016-07-20   \n",
      "2  4XFL2NAR75V6CQIG 2021-01-03   222.0  IBU3QNDABEVUQA7J     2016-07-20   \n",
      "3  4XFL2NAR75V6CQIG 2021-01-04   240.0  IBU3QNDABEVUQA7J     2016-07-20   \n",
      "4  4XFL2NAR75V6CQIG 2021-01-05   224.0  IBU3QNDABEVUQA7J     2016-07-20   \n",
      "\n",
      "         MARCA_COMP CODI_MODEL  DIAM_COMP  YEAR  \n",
      "0  5557SZ47QZAZ56EQ         31       15.0  2021  \n",
      "1  5557SZ47QZAZ56EQ         31       15.0  2021  \n",
      "2  5557SZ47QZAZ56EQ         31       15.0  2021  \n",
      "3  5557SZ47QZAZ56EQ         31       15.0  2021  \n",
      "4  5557SZ47QZAZ56EQ         31       15.0  2021  \n",
      "\n",
      "--- Final DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14439 entries, 0 to 14438\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   POLISSA_SUBM    14439 non-null  object        \n",
      " 1   DATA            14439 non-null  datetime64[ns]\n",
      " 2   CONSUM          14439 non-null  float64       \n",
      " 3   NUM_COMPLET     14439 non-null  object        \n",
      " 4   DATA_INST_COMP  14439 non-null  datetime64[ns]\n",
      " 5   MARCA_COMP      14439 non-null  object        \n",
      " 6   CODI_MODEL      14439 non-null  object        \n",
      " 7   DIAM_COMP       14439 non-null  float64       \n",
      " 8   YEAR            14439 non-null  int32         \n",
      "dtypes: datetime64[ns](2), float64(2), int32(1), object(4)\n",
      "memory usage: 959.0+ KB\n",
      "\n",
      "--- Pipeline Finished ---\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "print(\"--- Starting Data Processing Pipeline ---\")\n",
    "\n",
    "# --- 0. Load Data (as in data_analysis.ipynb) ---\n",
    "\n",
    "consum_dades = pd.read_parquet(\"Mostra Set de dades 4_Incidències en comptadors intel·ligents_Consum.parquet\")\n",
    "perimetre_dades = pd.read_parquet(\"Mostra Set de dades 4_Incidències en comptadors intel·ligents_Perímetre.parquet\")\n",
    "print(\"✓ Step 0: Data loaded successfully.\")\n",
    "print(f\"  - 'consum_dades' shape: {consum_dades.shape}\")\n",
    "print(f\"  - 'perimetre_dades' shape: {perimetre_dades.shape}\")\n",
    "\n",
    "\n",
    "if not consum_dades.empty and not perimetre_dades.empty:\n",
    "\n",
    "    # --- 1. Merge Datasets ---\n",
    "    # Merge consumption time series with technical meter information\n",
    "    # Using 'inner' join to ensure relational consistency\n",
    "    print(\"\\n--- Step 1: Merging Datasets ---\")\n",
    "    unified_df = pd.merge(\n",
    "        consum_dades,\n",
    "        perimetre_dades,\n",
    "        on=\"POLISSA_SUBM\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    print(f\"✓ Datasets merged on 'POLISSA_SUBM'. New shape: {unified_df.shape}\")\n",
    "\n",
    "    # --- 2. Verify Integrity and Clean Data ---\n",
    "    print(\"\\n--- Step 2: Verifying Integrity and Cleaning ---\")\n",
    "    \n",
    "    # Check for missing values introduced during the merge (should be 0)\n",
    "    missing_vals = unified_df.isnull().sum().sum()\n",
    "    print(f\"  - Total missing values: {missing_vals}\")\n",
    "    \n",
    "    # Remove duplicated time series records (same meter, same date)\n",
    "    initial_rows = unified_df.shape[0]\n",
    "    unified_df.drop_duplicates(subset=['POLISSA_SUBM', 'DATA'], keep='first', inplace=True)\n",
    "    rows_after_dupes = unified_df.shape[0]\n",
    "    print(f\"  - Removed {initial_rows - rows_after_dupes} duplicate records (same meter & date).\")\n",
    "    \n",
    "    # Remove invalid records (e.g., negative consumption)\n",
    "    initial_rows = unified_df.shape[0]\n",
    "    unified_df = unified_df[unified_df['CONSUM'] >= 0]\n",
    "    rows_after_invalid = unified_df.shape[0]\n",
    "    print(f\"  - Removed {initial_rows - rows_after_invalid} invalid records (negative consumption).\")\n",
    "    \n",
    "    print(f\"✓ Data cleaned. Final shape: {unified_df.shape}\")\n",
    "\n",
    "    # --- 3. Assess Completeness ---\n",
    "    print(\"\\n--- Step 3: Assessing Completeness ---\")\n",
    "    \n",
    "    # Convert DATA column to datetime for time-based analysis\n",
    "    # The 'DATA' column was identified as 'object' in the EDA\n",
    "    try:\n",
    "        unified_df['DATA'] = pd.to_datetime(unified_df['DATA'])\n",
    "        unified_df['DATA_INST_COMP'] = pd.to_datetime(unified_df['DATA_INST_COMP'])\n",
    "        print(\"✓ Converted 'DATA' and 'DATA_INST_COMP' to datetime objects.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not convert date columns: {e}\")\n",
    "\n",
    "    # Assess completeness across years\n",
    "    if 'DATA' in unified_df.columns:\n",
    "        min_date = unified_df['DATA'].min()\n",
    "        max_date = unified_df['DATA'].max()\n",
    "        print(f\"\\n  - Daily consumption data ranges from: {min_date.date()} to {max_date.date()}\")\n",
    "        \n",
    "        unified_df['YEAR'] = unified_df['DATA'].dt.year\n",
    "        print(\"\\n  - Record count by year:\")\n",
    "        print(unified_df['YEAR'].value_counts().sort_index())\n",
    "\n",
    "    # --- 5. Final Unified Dataset ---\n",
    "    print(\"\\n--- Step 5: Final Output ---\")\n",
    "    unified_clean_df = unified_df # Assign to the final variable name\n",
    "    print(\"✓ Unified, clean, and enhanced dataset is ready in the 'unified_clean_df' DataFrame.\")\n",
    "    \n",
    "    print(\"\\n--- Final DataFrame Head ---\")\n",
    "    print(unified_clean_df.head())\n",
    "    \n",
    "    print(\"\\n--- Final DataFrame Info ---\")\n",
    "    unified_clean_df.info()\n",
    "\n",
    "else:\n",
    "    print(\"\\nPipeline skipped as one or more data files could not be loaded.\")\n",
    "\n",
    "print(\"\\n--- Pipeline Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
